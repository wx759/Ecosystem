# import tensorflow as tf
import copy
import gc
import json
from fileinput import filename
from datetime import datetime
import numpy as np
import os
import random
import torch

import swanlab as wandb
from Agent.Config_PPO import Config_PPO
from real_System_remake.Bank_config import Bank_config
from real_System_remake.Enterprise_config import Enterprise_config
from real_System_remake.Environment import Environment
from real_System_remake.ppo_bank import bank_nnu
from real_System_remake.ppo_enterprise import enterprise_nnu
import torch
from torch.distributions import Normal
import torch.nn.functional as F

use_wandb = True
use_rbtree = False
lim_day = 500
enterprise_ppo_config = Config_PPO(
    scope='',
    state_dim=0,
    action_dim=4,
    hidden_dim=128,
    # ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶å†åˆå§‹åŒ–agentï¼Œä»¥ä¾¿åŠ¨æ€é€‚åº”çŠ¶æ€ç©ºé—´)
)

bank_ppo_config = Config_PPO(
    scope='',
    state_dim=0,
    action_dim=2,
    hidden_dim=128,

)

bank_config = Bank_config(
    name='bank1',
    fund=2000,
    fund_rate=1,
    fund_increase=0.1,
    debt_time=5
)

enterprise_config = Enterprise_config(
    name='',
    output_name='',
    price=8.0, intention=5.0)

# ä¸¤ä¸ªä¼ä¸šï¼Œä¸€ä¸ªç”Ÿäº§Kï¼Œä¸€ä¸ªç”Ÿäº§L
enterprise_add_list = {
    'production1': 'K',
    'consumption1': 'L'
}


class System:
    def __init__(self):
        self.env = Environment(name='PPO', lim_day=lim_day)

        for key in enterprise_add_list:
            config = copy.deepcopy(enterprise_config)
            config.name = key
            config.output_name = enterprise_add_list[key]
            self.env.add_enterprise_agent(config=config)
        self.env.add_bank(bank_config)
        self.env.add_enterprise_thirdmarket(name='production_thirdMarket', output_name='K', price=100)
        self.env.add_enterprise_thirdmarket(name='consumption_thirdMarket', output_name='L', price=100)

        self.env.init()
        # self.epiday=0 #å›åˆæ•°ï¼Œåœ¨ç®—æ³•å¤ªåƒåœ¾çš„æ—¶å€™å¯ä»¥æå‰ç»“æŸã€‚
        self.e_execute = self.env.get_enterprise_execute()
        self.b_execute = self.env.get_bank_execute()
        self.execute = self.e_execute + self.b_execute
        self.Agent = {}
        for key in self.execute:
            self.Agent[key] = None

    def run(self):
        seed = random.randint(0, 1000)
        config = Config_PPO(scope='', state_dim=0, action_dim=0, hidden_dim=0)
        wandb.init(project="TD3_vs_PPO", workspace="wx829", config={
            "random_seed": seed,
            "is_rms_state": config.is_rms_state,
            "is_rms_reward": config.is_rms_reward,
            "max_training_steps": config.MAX_TRAINING_STEPS,
            "total_step": config.total_step,
            "learning_rate_actor_enterprise": config.LEARNING_RATE_AC_Enterprise,
            "learning_rate_actor_bank": config.LEARNING_RATE_AC_Bank,
            "learning_rate_critic_enterprise": config.LEARNING_RATE_C_Enterprise,
            "learning_rate_critic_bank": config.LEARNING_RATE_C_Bank,
            "entropyRC_Enterprise": config.entropyRC_Enterprise,
            "entropyRC_Bank": config.entropyRC_Bank,
            "clip_range": config.CLIP_RANGE,
            "epoch": config.N_EPOCHS,
            "mini_batch": config.MINI_BATCH_SIZE,
            "update_timestep": config.UPDATE_TIMESTEP,
            "total_update": config.MAX_TRAINING_STEPS / config.UPDATE_TIMESTEP,
            "lim-day": lim_day
        })
        # 1. PPO è¶…å‚æ•°
        update_timestep = config.UPDATE_TIMESTEP
        # max_training_timesteps = config.MAX_TRAINING_STEPS
        total_step =config.total_step
        # 2. åˆå§‹åŒ–æ™ºèƒ½ä½“
        _temp_state = self.env.reset()
        for target_key in self.e_execute:
            if self.Agent[target_key] is None:
                config = copy.deepcopy(enterprise_ppo_config)
                config.set_scope(target_key)
                config.set_seed(seed)
                config.set_state_dim(len(_temp_state[target_key]))
                self.Agent[target_key] = enterprise_nnu(config)
        for target_key in self.b_execute:
            if self.Agent[target_key] is None:
                config = copy.deepcopy(bank_ppo_config)
                config.set_scope(target_key)
                config.set_seed(seed)
                config.set_state_dim(len(_temp_state[target_key]))
                self.Agent[target_key] = bank_nnu(config)

        # self.load_actor_only(save_dir="actors_only", note="")
        # 3. å¼€å§‹è®­ç»ƒå¾ªç¯
        state = self.env.reset()
        time_step = 0
        episode_num = 0

        while time_step < total_step:

            # --- æ•°æ®æ”¶é›†é˜¶æ®µ ---
            for _ in range(update_timestep):
                time_step += 1
                action, log_prob = {}, {}

                for target_key in self.e_execute:
                    act, lp = self.Agent[target_key].choose_action(state[target_key])
                    action[target_key], log_prob[target_key] = act, lp
                for target_key in self.b_execute:
                    act, lp = self.Agent[target_key].choose_action(state[target_key])
                    action[target_key], log_prob[target_key] = act, lp

                self.env.step(action)
                next_state, reward, done_env,info = self.env.observe()

                # NEW: ä¸ºæ¯ä¸ª agent è®¡ç®— next_value = V(next_state)
                next_v = {}
                for k in self.e_execute:
                    next_v[k] = self.Agent[k].get_value(next_state[k])
                for k in self.b_execute:
                    next_v[k] = self.Agent[k].get_value(next_state[k])

                # NEW: åŒæ©ç 
                is_terminal_for_gae = bool(info.get('terminated', done_env))  # è‡ªç„¶ç»ˆæ­¢æ‰æˆªæ–­ bootstrap
                nonterminal = 0 if done_env else 1  # ç»“æŸ(terminated æˆ– truncated)åˆ™æ–­å¼€ GAE é€’æ¨

                # CHANGED: å­˜ transitionï¼ˆå« next_value ä¸ nonterminalï¼‰
                for target_key in self.e_execute:
                    self.Agent[target_key].store_transition(
                        state[target_key],
                        action[target_key],
                        log_prob[target_key],
                        reward[target_key]['business'],
                        is_terminal_for_gae,
                        next_v[target_key],
                        nonterminal,
                    )
                for target_key in self.b_execute:
                    self.Agent[target_key].store_transition(
                        state[target_key],
                        action[target_key],
                        log_prob[target_key],
                        reward[target_key]['WNDB'],
                        is_terminal_for_gae,
                        next_v[target_key],
                        nonterminal,
                    )

                state = next_state

                if done_env:
                    state = self.env.reset()
                    episode_num += 1
                    print(f"Episode {episode_num} finished. Total timesteps: {time_step}")

            # --- å­¦ä¹ é˜¶æ®µ ---
            print(f"--- Timestep {time_step}. Updating policies... ---")

            for agent_key, agent in self.Agent.items():
                agent.learn(state[agent_key])
                agent.clear_memory()

            if use_wandb:
                critic_bank, actor_bank, avg_entropy_bank, clip_fraction_bank = self.Agent['bank1'].log()
                critic_production1, actor_production1, avg_entropy_production1, clip_fraction_production1 = self.Agent[
                    'production1'].log()
                critic_consumption1, actor_consumption1, avg_entropy_consumption1, clip_fraction_consumption1 = \
                    self.Agent['consumption1'].log()
                wandb.log({'actor_loss/bank1': actor_bank})
                wandb.log({'actor_loss/production1': actor_production1})
                wandb.log({'actor_loss/consumption1': actor_consumption1})

                wandb.log({'critic_loss/bank': critic_bank})
                wandb.log({'critic_loss/production1': critic_production1})
                wandb.log({'critic_loss/consumption1': critic_consumption1})

                wandb.log({'avg_entropy/bank': avg_entropy_bank})
                wandb.log({'avg_entropy/production1': avg_entropy_production1})
                wandb.log({'avg_entropy/consumption1': avg_entropy_consumption1})

                wandb.log({'clip_fraction/bank': clip_fraction_bank})
                wandb.log({'clip_fraction/production1': clip_fraction_production1})
                wandb.log({'clip_fraction/consumption1': clip_fraction_consumption1})

            print("--- Update finished. ---")

        print("--- Training finished. Starting evaluation... ---")
        # for i in range(100):
        #     avg_len_det = self.evaluate_policy(episodes=10)
        #     wandb.log({"eval/final_avg_len_det": avg_len_det})

        # self.env.finish()

    @staticmethod
    def set_seed(seed):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        os.environ["PYTHONHASHSEED"] = str(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        print(f"[INFO] Seed set to {seed}")

    # def evaluate_policy(self, episodes=10):
    #     lengths = []
    #
    #     for ep in range(episodes):
    #         state = self.env.reset()
    #         done = False
    #         steps = 0
    #
    #         while not done:
    #             action = {}
    #
    #             # å’Œ run ä¸€æ ·ï¼ŒæŒ‰ target_key å– state çš„å­éƒ¨åˆ†
    #             for target_key in self.e_execute:
    #                 act = self.Agent[target_key].choose_action_deterministic(
    #                     state[target_key]
    #                 )
    #                 action[target_key] = act
    #
    #             for target_key in self.b_execute:
    #                 act= self.Agent[target_key].choose_action_deterministic(
    #                     state[target_key]
    #                 )
    #                 action[target_key] = act
    #
    #             # ç¯å¢ƒäº¤äº’
    #             self.env.step(action)
    #             next_state, reward, done = self.env.observe()
    #             steps += 1
    #
    #         lengths.append(steps)
    #     avg_len = np.mean(lengths)
    #     return avg_len
    def evaluate_policy(self, episodes=20, deterministic=False, threshold=None, use_ema=False):
        """
        å¯¹å½“å‰è®­ç»ƒå¥½çš„ç­–ç•¥è¿›è¡Œè¯„ä¼°ã€‚
        å‚æ•°ï¼š
            episodes: è¯„ä¼°å›åˆæ•°
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆmuï¼‰
            threshold: è‹¥æŸå›åˆå­˜æ´»å¤©æ•° < thresholdï¼Œåˆ™è®°å½•å¤±è´¥è½¨è¿¹
            use_ema: æ˜¯å¦ä½¿ç”¨ EMA å¹³æ»‘æƒé‡ï¼ˆå¯é€‰ï¼‰
        """

        results = []
        failure_records = []

        # if use_ema:
        #     for key in self.Agent:
        #         if hasattr(self.Agent[key], "ema_params"):
        #             load_ema_to_model(self.Agent[key].actor, self.Agent[key].ema_params)

        print(f"\nå¼€å§‹è¯„ä¼°: episodes={episodes}, deterministic={deterministic}, EMA={use_ema}")

        for ep in range(episodes):
            state = self.reset_with_noise()
            done = False
            day = 1
            trajectory = []

            while not done:
                action = {}

                # ======================
                # ä¸ºæ¯ç±» Agent é€‰æ‹©åŠ¨ä½œ
                # ======================
                for target_key in self.e_execute:
                    act = self.Agent[target_key].choose_action_deterministic(state[target_key])
                    action[target_key] = act

                for target_key in self.b_execute:
                    act = self.Agent[target_key].choose_action_deterministic(state[target_key])
                    action[target_key] = act

                # ç¯å¢ƒæ­¥è¿›
                self.env.step(action)
                next_state, reward, done = self.env.observe()

                trajectory.append({
                    "day": day,
                    "state": {k: state[k].tolist() if hasattr(state[k], 'tolist') else state[k] for k in state},
                    "action": {k: action[k].tolist() for k in action},
                    "reward": reward
                })

                state = next_state
                day += 1

            results.append(day)
            print(f"  Episode {ep + 1}: survived {day} days")

            # è®°å½•å¤±è´¥è½¨è¿¹
            if threshold and day < threshold:
                failure_records.append(trajectory)

        # ========== æ±‡æ€»ç»Ÿè®¡ ==========
        results = np.array(results)
        res = {
            "mean": float(np.mean(results)),
            "std": float(np.std(results)),
            "median": float(np.median(results)),
            "min": int(np.min(results)),
            "max": int(np.max(results)),
            "pct5": float(np.percentile(results, 5)),
            "pct95": float(np.percentile(results, 95))
        }

        print("\n=== è¯„ä¼°ç»“æœ ===")
        for k, v in res.items():
            print(f"{k}: {v}")

        # ========== ä¿å­˜å¤±è´¥è½¨è¿¹ ==========
        if failure_records:
            filename = f"failures_eval_seed_200{self.seed if hasattr(self, 'seed') else 0}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(failure_records, f, ensure_ascii=False, indent=2)
            print(f"å¤±è´¥è½¨è¿¹å·²ä¿å­˜åˆ° {filename}")

        return res

    def collect_state_statistics(self, episodes=20):
        """
        ç”¨æœ€ç»ˆç­–ç•¥é‡‡æ ·è‹¥å¹²æ¡è½¨è¿¹ï¼Œè¿”å›æ¯ä¸ª agent æ¯ç»´ mean å’Œ stdï¼ˆflatten åï¼‰ã€‚
        ç»“æœæ ¼å¼ï¼šdict(agent_key -> {'mean': np.array, 'std': np.array})
        """
        all_states_per_agent = {k: [] for k in self.Agent.keys()}

        for ep in range(episodes):
            state = self.env.reset()
            done = False
            while not done:
                # éšæœº/ç¡®å®šåœ°ç”¨å½“å‰è®­ç»ƒå¥½ç­–ç•¥é‡‡æ ·åŠ¨ä½œï¼ˆå»ºè®®ä½¿ç”¨è®­ç»ƒæ—¶çš„é‡‡æ ·è¡Œä¸ºï¼‰
                action = {}
                for target_key in self.e_execute:
                    act, _ = self.Agent[target_key].choose_action(state[target_key])
                    action[target_key] = act

                for target_key in self.b_execute:
                    act = self.Agent[target_key].choose_action(state[target_key])
                    action[target_key] = act
                self.env.step(action)
                next_state, reward, done = self.env.observe()
                # æ”¶é›†æ¯ä¸ª agent çš„ stateï¼ˆæŒ‰ list/array æ ¼å¼ï¼‰
                for key in self.Agent.keys():
                    arr = np.array(next_state[key], dtype=np.float32).ravel()
                    all_states_per_agent[key].append(arr)
                state = next_state

        stats = {}
        for key, list_of_states in all_states_per_agent.items():
            if len(list_of_states) == 0:
                continue
            S = np.stack(list_of_states, axis=0)  # [T, D]
            stats[key] = {'mean': S.mean(axis=0), 'std': S.std(axis=0)}
        return stats

    def add_multiplicative_noise_to_state_vector(vec, std_vec, alpha):
        """
        vec: 1D numpy array (state flattened)
        std_vec: 1D numpy array same shape (per-dim std from collect)
        alpha: scalar factor (noise level relative to std)
        è¿”å›ï¼š noisy 1D array
        ä½¿ç”¨ä¹˜æ³•å™ªå£° s' = s * (1 + eps), eps ~ N(0, alpha * std_rel)
        è¿™é‡Œç”¨ç›¸å¯¹ std: std_rel = std / (|mean|+eps) ä¹Ÿå¯ç›´æ¥ç”¨ stdã€‚
        """
        eps_small = 1e-8
        # è‹¥ std_vec ä¸­æœ‰ 0ï¼Œé€€åŒ–åˆ°ä¸€ä¸ªå°å¸¸æ•°
        noise_sigma = alpha * (std_vec + eps_small)
        eps = np.random.normal(0.0, noise_sigma, size=vec.shape)
        return (vec * (1.0 + eps)).astype(np.float32)

    def reset_with_noise(self, noise_scale=0.2):
        """
        å¸¦åˆå§‹çŠ¶æ€æ‰°åŠ¨çš„ reset
        å¯¹çŠ¶æ€å‘é‡ä¸­çš„æ¯ä¸ª agent æ·»åŠ å¾®å°é«˜æ–¯å™ªå£°
        """
        state = self.env.reset()
        noisy_state = {}

        for key, value in state.items():
            # å¦‚æœæ˜¯ listï¼Œå…ˆè½¬æˆ np.array
            if isinstance(value, list):
                value = np.array(value, dtype=np.float32)

            # å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œæ·»åŠ å™ªå£°
            if isinstance(value, np.ndarray):
                noise = np.random.normal(0, noise_scale, size=value.shape)
                noisy_value = value + noise
                noisy_state[key] = noisy_value.tolist()  # è½¬å› listï¼Œä¿è¯ç¯å¢ƒå…¼å®¹
            else:
                # å¯¹éæ•°ç»„ï¼ˆå¦‚æ ‡é‡ã€å­—å…¸ï¼‰ä¿æŒåŸæ ·
                noisy_state[key] = value

        return noisy_state

    def save_actors(self, save_dir="actors_only", note=""):
        """
        ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“çš„ Actor å‚æ•°ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = save_dir + "_" + timestamp
        os.makedirs(save_dir, exist_ok=True)
        for target_key in self.e_execute:
            agent = self.Agent[target_key]
            filename = f"{save_dir}/{agent.scope}_actor.pt"
            torch.save(agent.enterprise.actor.state_dict(), filename)
            print(f"[ğŸ¯] å·²ä¿å­˜ {agent.scope} çš„ actor è‡³ {filename}")

        for target_key in self.b_execute:
            agent = self.Agent[target_key]
            filename = f"{save_dir}/{agent.scope}_actor.pt"
            torch.save(agent.bank.actor.state_dict(), filename)
            print(f"[ğŸ¯] å·²ä¿å­˜ {agent.scope} çš„ actor è‡³ {filename}")

    def load_actor_only(self, save_dir = "actors_only",note=""):
        for target_key in self.e_execute:
            agent = self.Agent[target_key]
            path = os.path.join(save_dir, f"{agent.scope}_actor_{note}.pt")
            if os.path.exists(path):
                agent.enterprise.actor.load_state_dict(torch.load(path))
                agent.enterprise.actor.eval()
                print(f"[ğŸ¯] åŠ è½½ actor: {agent.scope}")

        for target_key in self.b_execute:
            agent = self.Agent[target_key]
            path = os.path.join(save_dir, f"{agent.scope}_actor_{note}.pt")
            if os.path.exists(path):
                agent.bank.actor.load_state_dict(torch.load(path))
                agent.bank.actor.eval()
                print(f"[ğŸ¯] åŠ è½½ actor: {agent.scope}")

if __name__ == '__main__':
    # for i in range(3):
    system = System()

    system.run()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # system.save_actors()
    # system.evaluate_policy(episodes=500, deterministic=False, threshold=180)
    del system
    gc.collect()
    # æ¸…ç©ºè®¡ç®—å›¾
    torch.nn.Module.dump_patches = True
    torch.cuda.empty_cache()

    # tf.reset_default_graph()
