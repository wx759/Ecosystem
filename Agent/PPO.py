import os
import random

import torch
import torch.nn.functional as F
from torch.distributions import Normal
from Agent import Config_PPO
import numpy as np

from Agent.RuningMeanStd import RunningMeanStd
import swanlab as wandb
from torch.optim.lr_scheduler import LinearLR


class PPO:
    """å®Œæ•´çš„PPOè®­ç»ƒå™¨ï¼Œå°è£…è®­ç»ƒé€»è¾‘å’Œç®—æ³•"""

    def __init__(self, config: Config_PPO):
        # ç¯å¢ƒåˆå§‹åŒ–

        self.set_global_seed(config.random_seed)
        self.scope = config.scope
        self.state_dim = config.state_dim
        self.action_dim = config.action_dim
        self.hidden_dim = config.hidden_dim

        # åˆå§‹åŒ– RunningMeanStd å®ä¾‹
        self.rms = RunningMeanStd(shape=self.state_dim)
        self.rms_reward = RunningMeanStd(shape=())
        # æ·»åŠ ä¸€ä¸ªæ§åˆ¶å½’ä¸€åŒ–å¼€å…³çš„æ ‡å¿—ï¼Œæ–¹ä¾¿ä½ æµ‹è¯•å¯¹æ¯”æ•ˆæœ
        self.is_rms = config.is_rms_state  # åˆå§‹è®¾ç½®ä¸º Trueï¼Œå¦‚æœä½ æƒ³æš‚æ—¶å…³é—­å¯ä»¥æ”¹ä¸º False
        self.is_rms_reward = config.is_rms_reward

        # ç¼“å†²åŒºåˆ—è¡¨
        self.states = []
        self.actions = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
        self.next_values = []  # NEW: V(next_state)
        self.nonterminal_masks = []  # NEW: 1 æœªç»“æŸï¼Œ0 done(terminated æˆ– truncated)
        # è¶…å‚æ•°
        self.gamma = config.GAMMA
        self.lamda = config.LAMDA
        self.epochs = config.N_EPOCHS
        self.eps = config.CLIP_RANGE
        self.mini_batch_size = config.MINI_BATCH_SIZE

        self.entropy_start_enterprise = config.entropyRC_Enterprise
        self.entropy_start_bank = config.entropyRC_Bank
        self.entropyRC_Enterprise = config.entropyRC_Enterprise
        self.entropyRC_Bank = config.entropyRC_Bank
        self.learning_rate_actor_bank = config.LEARNING_RATE_AC_Bank
        self.learning_rate_actor_enterprise = config.LEARNING_RATE_AC_Enterprise
        self.learning_rate_critic_bank = config.LEARNING_RATE_C_Bank
        self.learning_rate_critic_enterprise = config.LEARNING_RATE_C_Enterprise
        self.critic_loss = 0
        self.actor_loss = 0
        self.avg_entropy = 0.0
        self.avg_clip_frac = 0.0

        # è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        # self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = 'cpu'
        # åˆå§‹åŒ–ç½‘ç»œå’Œä¼˜åŒ–å™¨
        self.actor = PolicyNet(self.state_dim, self.hidden_dim, self.action_dim).to(self.device)
        self.critic = ValueNet(self.state_dim, self.hidden_dim).to(self.device)
        if config.scope == 'bank1':
            self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.learning_rate_actor_bank)
            self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.learning_rate_critic_bank)
        else:
            self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.learning_rate_actor_enterprise)
            self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.learning_rate_critic_enterprise)
        self.actor_scheduler = LinearLR(
            self.actor_optimizer,
            start_factor=1.0,
            end_factor=0.3,
            total_iters=config.total_update)

    def set_global_seed(self, seed):
        # pytorch_seed
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        # python_seed
        random.seed(seed)

        # NumPy_seed
        np.random.seed(seed)

        # è®¾ç½®pythonçš„hashéšæœºç§å­
        os.environ['PYTHONHASHSEED'] = str(seed)
        torch.backends.cudnn.deterministic = True
        # è®¾ç½®PyTorchä½¿ç”¨çš„ç®—æ³•ä¸ºç¡®å®šæ€§ç®—æ³•
        torch.backends.cudnn.benchmark = False

    def choose_action(self, state):

        state = torch.tensor(state, dtype=torch.float).unsqueeze(0).to(self.device)

        with torch.no_grad():
            mu, sigma = self.actor(state)
            dist = Normal(mu, sigma)
            raw_action = dist.sample()
            action = torch.tanh(raw_action) * 0.5
            log_prob = dist.log_prob(raw_action).sum(dim=-1)

            # åŠ å…¥ Jacobian ä¿®æ­£é¡¹
            log_det_jacobian = 2 * (torch.log(torch.tensor(2.0)) - raw_action - F.softplus(-2 * raw_action))
            log_det_jacobian = log_det_jacobian.sum(dim=-1)

            correction = self.action_dim * torch.log(torch.tensor(0.5))
            log_prob = log_prob - log_det_jacobian - correction

        return action.cpu().numpy().flatten(), log_prob.cpu().item()

    def store_transition(self, state, action, logprob, reward, is_terminal, next_value, nonterminal):  # CHANGED
        self.states.append(state)
        self.actions.append(action)
        self.logprobs.append(logprob)
        self.rewards.append(reward)
        self.is_terminals.append(is_terminal)  # ä»…è‡ªç„¶ç»ˆæ­¢
        self.next_values.append(next_value)  # V(next_state)
        self.nonterminal_masks.append(nonterminal)  # é€’æ¨æ˜¯å¦ç»§ç»­

    def clear_memory(self):
        del self.states[:]
        del self.actions[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]
        del self.next_values[:]  # NEW
        del self.nonterminal_masks[:]  # NEW

    def get_value(self, state):
        # 1. å°†å•ä¸ªçŠ¶æ€è½¬æ¢ä¸º (1, state_dim) çš„ NumPy æ•°ç»„
        state_np = np.array(state, dtype=np.float64)[np.newaxis, :]

        # 2. å¯¹çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ï¼ˆä¸ choose_action å’Œ learn ä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
        if self.is_rms:
            state_np = self.rms.normalize(state_np)

        # 3. è½¬æ¢ä¸º PyTorch å¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        state_tensor = torch.tensor(state_np, dtype=torch.float).to(self.device)

        with torch.no_grad():
            value = self.critic(state_tensor).item()  # Critic æ¥æ”¶å½’ä¸€åŒ–åçš„çŠ¶æ€
        return value

    def learn(self, final_state, agent_type: str):
        # 1. ä»ç¼“å†²åŒºè·å–å¹¶å¤„ç†çŠ¶æ€æ•°æ®
        raw_old_states_np = np.array(self.states, dtype=np.float32)
        if self.is_rms:
            normalized_old_states_np = self.rms.normalize(raw_old_states_np)
        else:
            normalized_old_states_np = raw_old_states_np
        # if self.is_rms_reward and agent_type != 'bank1':
        if self.is_rms_reward:
            rewards_np = np.array(self.rewards, dtype=np.float32)
            rewards_np = rewards_np / (rewards_np.std() + 1e-8)
            self.rewards = rewards_np.tolist()

        old_states_tensor = torch.tensor(normalized_old_states_np, dtype=torch.float).to(self.device)
        old_actions = torch.tensor(np.array(self.actions), dtype=torch.float).to(self.device)
        old_logprobs = torch.tensor(self.logprobs, dtype=torch.float).to(self.device)

        # GAE è®¡ç®—
        state_values = self.critic(old_states_tensor).squeeze().detach()
        values = state_values.tolist()

        next_values = np.array(self.next_values, dtype=np.float32)
        is_terminals_np = np.array(self.is_terminals, dtype=np.float32)
        nonterminal_masks_np = np.array(self.nonterminal_masks, dtype=np.float32)

        advantages = []
        gae = 0.0
        for t in reversed(range(len(self.rewards))):
            # è‡ªç„¶ç»ˆæ­¢ç¦ç”¨ bootstrapï¼›æˆªæ–­å…è®¸ bootstrap
            bootstrap_next_v = (1.0 - is_terminals_np[t]) * float(next_values[t])
            delta = self.rewards[t] + self.gamma * bootstrap_next_v - values[t]
            # doneï¼ˆterminated æˆ– truncatedï¼‰å‡åœæ­¢é€’æ¨
            gae = delta + self.gamma * self.lamda * nonterminal_masks_np[t] * gae
            advantages.insert(0, gae)

        advantages = torch.tensor(advantages, dtype=torch.float).to(self.device)

        returns = (advantages + state_values).detach()

        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Mini-Batch è®­ç»ƒé˜¶æ®µ
        batch_size_total = len(self.states)
        for epoch in range(self.epochs):
            # åˆ›å»ºåˆ—è¡¨ä»¥è®°å½•æ¯ä¸ªmini-batchçš„æŸå¤±
            actor_loss_list = []
            critic_loss_list = []
            entropy_list, clip_frac_list = [], []

            shuffled_indices = torch.randperm(batch_size_total)
            for start_idx in range(0, batch_size_total, self.mini_batch_size):
                end_idx = min(start_idx + self.mini_batch_size, batch_size_total)
                batch_indices = shuffled_indices[start_idx:end_idx]
                mini_batch_old_states = old_states_tensor[batch_indices]
                mini_batch_old_actions = old_actions[batch_indices]
                mini_batch_old_logprobs = old_logprobs[batch_indices]
                mini_batch_advantages = advantages[batch_indices]
                mini_batch_returns = returns[batch_indices]

                mu, std = self.actor(mini_batch_old_states)

                logprobs = self.compute_log_prob(mu, std, mini_batch_old_actions)
                ratios = torch.exp(logprobs - mini_batch_old_logprobs.detach())
                clip_frac = ((ratios > 1 + self.eps) | (ratios < 1 - self.eps)).float().mean().item()
                clip_frac_list.append(clip_frac)

                surr1 = ratios * mini_batch_advantages
                surr2 = torch.clamp(ratios, 1 - self.eps, 1 + self.eps) * mini_batch_advantages
                actor_loss_per_batch = -torch.min(surr1, surr2).mean()

                dist = torch.distributions.Normal(mu, std)
                entropy = dist.entropy().mean().item()
                entropy_list.append(entropy)

                if agent_type == 'bank1':
                    entropy_loss = -self.entropyRC_Bank * dist.entropy().mean()
                else:
                    entropy_loss = -self.entropyRC_Enterprise * dist.entropy().mean()

                actor_loss_with_entropy = actor_loss_per_batch + entropy_loss

                current_values = self.critic(mini_batch_old_states).view(-1)
                critic_loss_per_batch = F.mse_loss(current_values, mini_batch_returns.view(-1))

                # è®°å½•æ¯ä¸ªmini-batchçš„æŸå¤±
                actor_loss_list.append(actor_loss_per_batch.item())
                critic_loss_list.append(critic_loss_per_batch.item())

                # åœ¨æ¯æ¬¡ Mini-Batch çš„è®­ç»ƒå¼€å§‹å‰ï¼Œå°†ä¼˜åŒ–å™¨ä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦æ¸…é›¶ã€‚
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()

                # åå‘ä¼ æ’­ è®¡ç®—æŸå¤±å¯¹å„è‡ªç½‘ç»œå‚æ•°çš„æ¢¯åº¦ã€‚å¹¶å°†å…¶å­˜å‚¨åœ¨gradå±æ€§
                actor_loss_with_entropy.backward()
                critic_loss_per_batch.backward()

                # æ¢¯åº¦è£å‰ª é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=0.5)

                # è¯»å–.gradå­˜å‚¨çš„æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°
                self.actor_optimizer.step()
                self.critic_optimizer.step()


            # ã€ä¿®æ”¹åã€‘ä¿å­˜æ‰€æœ‰mini-batchæŸå¤±çš„å¹³å‡å€¼ï¼Œç”¨äºæ—¥å¿—è®°å½•
            self.actor_loss = np.mean(actor_loss_list)
            self.critic_loss = np.mean(critic_loss_list)

            self.avg_entropy = np.mean(entropy_list)
            self.avg_clip_frac = np.mean(clip_frac_list)
            # ğŸ‘‡ ===== KLæ—©åœæœºåˆ¶ ===== ğŸ‘‡
          #  with torch.no_grad():
                # ç”¨æ›´æ–°åçš„ç½‘ç»œé‡æ–°è®¡ç®—æ•´ä¸ªbatchçš„log_prob
               # mu_new, std_new = self.actor(old_states_tensor)
             #   new_logprobs = self.compute_log_prob(mu_new, std_new, old_actions)

                # è®¡ç®—KLæ•£åº¦ï¼ˆè¿‘ä¼¼ï¼‰
                # D_KL(Ï€_old || Ï€_new) â‰ˆ E[log Ï€_old - log Ï€_new]
               # approx_kl = (old_logprobs - new_logprobs).mean().item()

                # å¦‚æœKLæ•£åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæå‰åœæ­¢
                #if approx_kl > 1:  # é˜ˆå€¼å¯è°ƒ
                   # print(f"[{agent_type}] Epoch {epoch + 1}/{self.epochs}: "
                       #   f"KL={approx_kl:.4f} > 0.1, æ—©åœ")
                  #  break
        if self.actor_scheduler.last_epoch < self.actor_scheduler.total_iters:
            self.actor_scheduler.step()

        # === æ¨¡æ‹Ÿé€€ç«å¼ ç†µè¡°å‡ ===
        progress = self.actor_scheduler.last_epoch / self.actor_scheduler.total_iters
        decay_factor = max(0.0, 1.0 - progress)
        self.entropyRC_Enterprise = self.entropy_start_enterprise * (0.32 + 0.68 * decay_factor)
        self.entropyRC_Bank = self.entropy_start_bank * (0.32 + 0.68 * decay_factor)
        # ã€æ–°å¢ã€‘è¯Šæ–­
        self.diagnose(old_states_tensor, old_actions, old_logprobs, agent_type)

    def get_loss(self):
        return self.critic_loss, self.actor_loss

    def get_test_indicator(self):
        return self.avg_entropy, self.avg_clip_frac

    @staticmethod
    def compute_log_prob(mu, std, action):
        dist = Normal(mu, std)
        # åŠ¨ä½œè¢«é™åˆ¶åœ¨ [-0.5, 0.5] ä¹‹é—´, é€†å˜æ¢æ˜¯ * 2
        scaled_action = torch.clamp(action * 2.0, -1.0 + 1e-6, 1.0 - 1e-6)
        raw_action = torch.atanh(scaled_action)

        log_prob_raw = dist.log_prob(raw_action)
        log_det_jacobian = 2 * (torch.log(torch.tensor(2.0)) - raw_action - F.softplus(-2 * raw_action))

        # ã€ä¿®æ”¹åã€‘ä¸ choose_action ä¸­ä¸€æ ·ï¼Œè¡¥ä¸Šå¯¹æ•°ç¼©æ”¾å› å­ log(0.5)ã€‚
        # è¿™é‡Œçš„å¼ é‡è¿ç®—æ˜¯å…ƒç´ çº§åˆ«çš„ï¼Œæ‰€ä»¥ç›´æ¥å‡å»æ ‡é‡å³å¯ï¼ŒPyTorchä¼šè‡ªåŠ¨å¹¿æ’­ã€‚
        log_prob = log_prob_raw - log_det_jacobian - torch.log(torch.tensor(0.5))

        return log_prob.sum(-1)

    def diagnose(self, old_states_tensor, old_actions, old_logprobs, agent_type):
        """è¯Šæ–­ PPO è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æŒ‡æ ‡"""
        with torch.no_grad():
            # 1. Actor åˆ†å¸ƒå‚æ•°
            mu, std = self.actor(old_states_tensor)
            #è®­ç»ƒåçš„ç­–ç•¥ï¼Œå¯¹è®­ç»ƒæ•°æ®ï¼ˆæ—§çŠ¶æ€ï¼‰é‡æ–°é¢„æµ‹æ—¶çš„æ ‡å‡†å·®å‡å€¼
            log_std_mean = torch.log(std).mean().item()
            v_pred = self.critic(old_states_tensor).squeeze(-1)
            #è®­ç»ƒåçš„ä»·å€¼ç½‘ç»œï¼Œå¯¹è®­ç»ƒæ•°æ®ï¼ˆæ—§çŠ¶æ€ï¼‰é‡æ–°ä¼°å€¼æ—¶çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
            v_mean =v_pred.mean().item()
            v_std = v_pred.std().item()

            # 3. KL divergenceï¼ˆæ–°æ—§ç­–ç•¥åˆ†å¸ƒå·®å¼‚ï¼‰
            new_logprobs = self.compute_log_prob(mu, std, old_actions)
            kl = (old_logprobs - new_logprobs).mean().item()
            if agent_type == 'production1':
                wandb.log({
                    "log_std_mean/production1": log_std_mean,
                    "KL_divergence/production1": kl,
                    "critic_mean/production1": v_mean,
                    "critic_std/production1": v_std,
                })
            elif agent_type == 'consumption1':
                wandb.log({
                    "log_std_mean/consumption1": log_std_mean,
                    "KL_divergence/consumption1": kl,
                    "critic_mean/consumption1": v_mean,
                    "critic_std/consumption1": v_std,
                })
            else:
                wandb.log({
                    "log_std_mean/bank": log_std_mean,
                    "KL_divergence/bank": kl,
                    "critic_mean/bank": v_mean,
                    "critic_std/bank": v_std,
                })

    def choose_action_deterministic(self, state):
        state = torch.tensor(state,dtype=torch.float).unsqueeze(0).to(self.device)

        with torch.no_grad():
            mu, sigma = self.actor(state)
            raw_action = mu
            # çº¦æŸåŠ¨ä½œèŒƒå›´ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
            action = torch.tanh(raw_action) * 0.5

        return action.cpu().numpy().flatten()


class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super().__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)

        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)
        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        raw_mu = self.fc_mu(x)
        raw_std = torch.clamp(self.fc_std(x), -20, 2)
        std = F.softplus(raw_std) + 0.01  # åŠ ä¸€ä¸ªå°çš„åç½®ï¼Œé˜²æ­¢ sigma å˜ä¸º 0
        return raw_mu, std


class ValueNet(torch.nn.Module):
    """ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰"""

    def __init__(self, state_dim, hidden_dim):
        super().__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
